
[Learn to Unlearn: Insights into Machine Unlearing](https://arxiv.org/abs/2305.07512): 

> Golatkar et al. introduced a forgetting Lagrangian to accomplish selective forgetting in DNNs. Building upon this concept, the authors devised a scrubbing method capable of erasing information from trained weights without the need to access the original training data or mandate a complete retraining of the network. Moreover, they developed a computable upper bound to measure the amount of residual information, a value that can be efficiently computed for DNNs. 
> 
> Nevertheless, when it pertains to forgetting without assuming prior training, research has revealed that even slight perturbations during the crucial learning phase can engender substantial variations in the eventual solution.